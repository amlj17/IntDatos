{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDd6UDb-YXt"
      },
      "source": [
        "### Carga de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WELmNut8LiJc",
        "outputId": "2e2a59b5-16b4-41da-b36c-2b4d78d43337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from lime) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from lime) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from lime) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.9/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.9/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (2023.4.12)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (2.25.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (8.4.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->lime) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283859 sha256=2063ade335be6d6263a7dfb8488ea128bd44a3b446b43a4df5c07e478b34fb41\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/d7/c9/5a0130d06d6310bc6cbe55220e6e72dcb8c4eff9a478717066\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUHp7NzP8lwf",
        "outputId": "e19b1f89-f1e4-4c4e-d0a8-8c4d5d3e6235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqFW-eNG7_Vb",
        "outputId": "4dc53633-17d0-436e-9e79-335aa500a4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=fafd438e0b19a6a79523b35e9e047f151227e04cb117d824719a7bfa88404773\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naCcDoX37mXA"
      },
      "outputs": [],
      "source": [
        "# for data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "# for processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import emoji\n",
        "import contractions \n",
        "\n",
        "# for ML\n",
        "from sklearn import feature_extraction, feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing, metrics, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, cross_validate, RepeatedStratifiedKFold, train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# for explainer\n",
        "from lime import lime_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LmMPv4RQywB",
        "outputId": "7bb9e2b1-315d-431c-cfb1-45001bdb2759"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Zz6Gc_-cVU"
      },
      "source": [
        "### Carga de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0tDK7YMT-L"
      },
      "source": [
        "Importamos los datos desde un archivo csv a un DataFrame de Pandas. En este caso, tengo los archivos en mi Google Drive en una carpeta llamada csv_files. Si no quiere montar el Drive, pueden subir el archivo como lo hicieron en las sesiones pasadas. Solo borren o comenten la celda donde se monta le discor y cambien la dirección del archivo en `pd.read_csv('<nombre-del-archivo>')`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANIK4tye63wT"
      },
      "source": [
        "Para esta práctica vamos a usar el conjunto de datos Disaster Tweets, el cual se encuentra dispoible en [Kaggle](https://www.kaggle.com/competitions/nlp-getting-started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha1cCHJd7toP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPaG_2P0M-1C"
      },
      "source": [
        "Renombramos los encabezados para no escribir tanto en el futuro y seguir una nomenclatura aceptada dentro del área de ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuV6guW2Mo5v"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={\"target\":\"y\", \"text\":\"text\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__miYJuzNH1g"
      },
      "source": [
        "Mandamos a imprimir 5 filas de forma aleatoria para observar los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "P4aFpt5bMZ8K",
        "outputId": "ce12f6d7-1b7f-4d96-ba07-e054b6ac8639"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8cf91390-37a3-45ff-a684-bdced60b0ce6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>570</td>\n",
              "      <td>arson</td>\n",
              "      <td>Eldoret, kenya</td>\n",
              "      <td>#Kisii Police in Kisii hunt for students over ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4777</th>\n",
              "      <td>6796</td>\n",
              "      <td>lightning</td>\n",
              "      <td>Waverly, IA</td>\n",
              "      <td>'When you walk away\\nNothing more to say\\nSee ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4588</th>\n",
              "      <td>6525</td>\n",
              "      <td>injuries</td>\n",
              "      <td>Toronto</td>\n",
              "      <td>All injuries Pre Foster/Floyd. Those will be c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>826</td>\n",
              "      <td>battle</td>\n",
              "      <td>Australia</td>\n",
              "      <td>#LonePine remembered around Australia as 'desc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6761</th>\n",
              "      <td>9687</td>\n",
              "      <td>tornado</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heather Night and Ava Sparxxx enjoy a wild tee...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cf91390-37a3-45ff-a684-bdced60b0ce6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cf91390-37a3-45ff-a684-bdced60b0ce6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cf91390-37a3-45ff-a684-bdced60b0ce6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id    keyword        location  \\\n",
              "395    570      arson  Eldoret, kenya   \n",
              "4777  6796  lightning     Waverly, IA   \n",
              "4588  6525   injuries         Toronto   \n",
              "571    826     battle       Australia   \n",
              "6761  9687    tornado             NaN   \n",
              "\n",
              "                                                   text  y  \n",
              "395   #Kisii Police in Kisii hunt for students over ...  1  \n",
              "4777  'When you walk away\\nNothing more to say\\nSee ...  0  \n",
              "4588  All injuries Pre Foster/Floyd. Those will be c...  0  \n",
              "571   #LonePine remembered around Australia as 'desc...  0  \n",
              "6761  Heather Night and Ava Sparxxx enjoy a wild tee...  0  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ0FQiD0Nhhh"
      },
      "source": [
        "Vamos a explorar brevemente la distribución de los datos según su clase por medio de un gráfico de barras horizontales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "JfbleufNNZcu",
        "outputId": "38286f40-8203-40da-d9aa-adc5aa3a8d59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.suptitle(\"Class balance\", fontsize=12)\n",
        "df[\"y\"].reset_index().groupby(\"y\").count().sort_values(by= \n",
        "       \"index\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dYYWO1-ib7"
      },
      "source": [
        "### Procesamiento de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWwN_ez9N8ry"
      },
      "source": [
        "A continuación, vamos a realizar una función para procesar el texto: limpieza, remover stopwords, y normalizar mediante lematización o stemming. Esta función se puede mapear en un DataFrame y esta forma, procesar toda una columna de texto. En este caso, añadí la propuesta hecha en el curso de procesamiento en la función `my_preprocess_text`.\n",
        "\n",
        "Nota: por el momento, solo es posible lematizar documentos en inglés. Para trabajar con documentos en español, es necesario realizar stemming. Para activar el proceso de lematización o stemming, cambien el valor de las banderas de `True` a `False`, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVU6nFzCptGW"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "\tif treebank_tag.startswith('J'):\n",
        "\t\treturn nltk.corpus.wordnet.ADJ\n",
        "\telif treebank_tag.startswith('V'):\n",
        "\t\treturn nltk.corpus.wordnet.VERB\n",
        "\telif treebank_tag.startswith('N'):\n",
        "\t\treturn nltk.corpus.wordnet.NOUN\n",
        "\telif treebank_tag.startswith('R'):\n",
        "\t\treturn nltk.corpus.wordnet.ADV\n",
        "\telse:\n",
        "\t\treturn nltk.corpus.wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexd-6Wilzy9"
      },
      "outputs": [],
      "source": [
        "def my_preprocess_text(text, flg_stemm=True, flg_lemm=False, lst_stopwords=None, flg_contractions = False):\n",
        "\n",
        "  # quitamos hashtags\n",
        "  # clean_tweet = re.sub('#[A-Za-z0-9_]+', '', text)\n",
        "\n",
        "  # quitamos cashtags\n",
        "  clean_tweet = re.sub('\\$[A-Za-z0-9_]+', '', text)\n",
        "\n",
        "  # quitamos nombres de usuario\n",
        "  clean_tweet = re.sub('@[A-Za-z0-9_]+', '', clean_tweet)\n",
        "\n",
        "  # sustituimos emojis por su traduccion al idioma objetivo\n",
        "  clean_tweet = emoji.demojize(clean_tweet, language = 'en')\n",
        "\n",
        "  # quitamos enlaces\n",
        "  clean_tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', clean_tweet)\n",
        "\n",
        "  # quitamos signos de puntuacion\n",
        "  clean_tweet = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', clean_tweet)\n",
        "\n",
        "  # quitamos numeros\n",
        "  clean_tweet = re.sub('[0-9_]+', '', clean_tweet)\n",
        "\n",
        "  # quitar caracteres repetidos\n",
        "  clean_tweet = re.sub(r'(.)\\1{2,}', r'\\1', clean_tweet)\n",
        "\n",
        "  # quitamos contracciones (solo para el ingles)\n",
        "  if (flg_contractions == True):\n",
        "    clean_tweet = contractions.fix(clean_tweet)\n",
        "\n",
        "  # tokenizamos\n",
        "  clean_tweet = word_tokenize(clean_tweet)\n",
        "\n",
        "  # filtramos palabras vacias\n",
        "  if (lst_stopwords is not None):\n",
        "    clean_tweet = [word for word in clean_tweet if word not in lst_stopwords]\n",
        "  \n",
        "  # Stemming \n",
        "  if (flg_stemm == True):\n",
        "    ps = nltk.stem.porter.PorterStemmer()\n",
        "    clean_tweet = [ps.stem(word) for word in clean_tweet]\n",
        "                \n",
        "  # Lematizacion\n",
        "  if flg_lemm == True:\n",
        "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    tags = nltk.pos_tag(clean_tweet)\n",
        "    clean_tweet = [wnl.lemmatize(j[0],get_wordnet_pos(j[1])) for j in tags]\n",
        "            \n",
        "  # volvemos a jutar las palabras en una oracion\n",
        "  text = \" \".join(clean_tweet).lower()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JdHwBkoRO-N"
      },
      "source": [
        "Aplicamos nuestra función para procesar el texto en el corpus dentro del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RSRiNksQd20"
      },
      "outputs": [],
      "source": [
        "df[\"text_clean\"] = df[\"text\"].apply(lambda x: my_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
        "                                                                      lst_stopwords=nltk.corpus.stopwords.words(\"english\"), flg_contractions = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "GUqHUgayQ8-9",
        "outputId": "f51395c9-a96f-43ae-ec28-2137d5f62898"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-631193ff-8d95-41ac-8010-5f517787416a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>y</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>our deeds reason earthquake may allah forgive u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>all resident ask shelter place notify officer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>people receive wildfire evacuation order calif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>just get sent photo ruby alaska smoke wildfire...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-631193ff-8d95-41ac-8010-5f517787416a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-631193ff-8d95-41ac-8010-5f517787416a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-631193ff-8d95-41ac-8010-5f517787416a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id keyword location                                               text  y  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...  1   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada  1   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...  1   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...  1   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...  1   \n",
              "\n",
              "                                          text_clean  \n",
              "0    our deeds reason earthquake may allah forgive u  \n",
              "1              forest fire near la ronge sask canada  \n",
              "2  all resident ask shelter place notify officer ...  \n",
              "3  people receive wildfire evacuation order calif...  \n",
              "4  just get sent photo ruby alaska smoke wildfire...  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nNMIvJd-nOu"
      },
      "source": [
        "### Preparación de los datos para el modelo de ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RxQ3OqBRWIP"
      },
      "source": [
        "Empezamos con las bases para entrenar los modelos. Primero, realizamos la partición del conjunto de datos en conjuntos de entrenamiento y prueba. Para obtener los valores de `y` debemos extraerlos mediante `values`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aprpAVQPRUzO"
      },
      "outputs": [],
      "source": [
        "# split train-test\n",
        "df_train, df_test = train_test_split(df, test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwSf5x8IAfjh"
      },
      "source": [
        "### Clasificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqWSA2w9eNFW"
      },
      "outputs": [],
      "source": [
        "def set_feature_extraction(method = 'bow', ngram = 2):\n",
        "  # regresa el metodo para vectorizar el texto: Bag-of-Words y TF-IDF con ngramas (1-2) o (1-3)\n",
        "  if (method=='bow'):\n",
        "    return feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,ngram))\n",
        "  elif (method == 'tfidf'):\n",
        "    return feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,ngram))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxYKS8ugeKWi"
      },
      "outputs": [],
      "source": [
        "def chi2_dim_reduction(df, y, method = 'tfidf', n = 2):\n",
        "\n",
        "  # extraemos el texto limpio para reducirlo via chi2\n",
        "  corpus = df[\"text_clean\"]\n",
        "\n",
        "  # se transforma segun tfidf\n",
        "  vectorizer = set_feature_extraction(method = 'tfidf', ngram = n)\n",
        "\n",
        "  vectorizer.fit(corpus) # realizamos la vectorizacion (bow con tfidf)\n",
        "  X = vectorizer.transform(corpus) #transformamos el conjunto de entrenamiento\n",
        "  dic_vocabulary = vectorizer.vocabulary_ # determinamos cual es nuestro vocabulario final\n",
        "\n",
        "  # reduccion de dimensionalidad via prueba chi2\n",
        "\n",
        "  X_names = vectorizer.get_feature_names_out()\n",
        "  p_value_limit = 0.95\n",
        "\n",
        "  # creamos un dataframe nuevo para albergar las palabras que pasa la prueba\n",
        "  df_features = pd.DataFrame()\n",
        "\n",
        "  for cat in np.unique(y):\n",
        "    chi2, p = feature_selection.chi2(X, y==cat) \n",
        "    df_features = df_features.append(pd.DataFrame(\n",
        "                     {\"feature\":X_names, \"score\":1-p, \"y\":cat})) \n",
        "    df_features = df_features.sort_values([\"y\",\"score\"], \n",
        "                      ascending=[True,False])\n",
        "    df_features = df_features[df_features[\"score\"]>p_value_limit]\n",
        "    \n",
        "  X_names = df_features[\"feature\"].unique().tolist()\n",
        "\n",
        "  # se crea un nuevo vectorizer utilizando el nuevo vocabulario reducido\n",
        "  red_vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
        "  red_vectorizer.fit(corpus)\n",
        "  X = red_vectorizer.transform(corpus) \n",
        "  dic_vocabulary = red_vectorizer.vocabulary_\n",
        "\n",
        "  return red_vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kuekvtGeAEF"
      },
      "outputs": [],
      "source": [
        "def grid_search_clf(df, C=None, gamma = None, search_delta=0.25):\n",
        "\n",
        "  # extraemos los valores 'y' para clasificar del dataframe que alberga los datos\n",
        "  y = df['y'].values\n",
        "\n",
        "  # reduccion de dimensionalidad via chi2\n",
        "  vectorizer = chi2_dim_reduction(df, y, n=3)\n",
        "  # vectorizer = set_feature_extraction(method = 'tfidf', ngram=3)\n",
        "\n",
        "  # parametros de la malla de busqueda\n",
        "  clf = svm.SVC(class_weight='balanced', probability=False)\n",
        "\n",
        "  # pipeline --- Update 04/07/22: new pipeline including tipologies count for each tweet/header\n",
        "  text_features = ColumnTransformer([(\"vectorizer\", vectorizer, 'text_clean')])\n",
        "  model = pipeline.Pipeline([(\"features\", text_features),  (\"classifier\", clf)])\n",
        "\n",
        "  if (C is None):\n",
        "    # Parametros de la malla: ojo con la definicion ->  classifier__ // dos guiones bajos antes del parametro\n",
        "    grid_params = {}\n",
        "    grid_params['classifier__kernel'] = ['rbf']\n",
        "    grid_params['classifier__C'] = [2**k for k in range(-5,16)] \n",
        "    grid_params['classifier__gamma'] = [2**k for k in range(-15, 4)]\n",
        "  else:\n",
        "    grid_params = {}\n",
        "    grid_params['classifier__kernel'] = ['rbf']\n",
        "    grid_params['classifier__C'] = [2**(C+(k*search_delta)) for k in range(-8,8)] # np.arange(0.1,4.1,0.1) \n",
        "    grid_params['classifier__gamma'] = [2**(gamma+(k*search_delta)) for k in range(-8,8)] #[2**(-7+(0.25*k)) for k in range(-8,8)] # np.arange(0.1,4.1,0.1)\n",
        "\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
        "  grid_search = GridSearchCV(model, param_grid=grid_params, n_jobs=-1, cv=cv, scoring='balanced_accuracy')\n",
        "  grid_result = grid_search.fit(df, y) # fit(X,y)\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  print('Best: %f, %f'% (grid_result.cv_results_['mean_test_score'][grid_result.best_index_], grid_result.cv_results_['std_test_score'][grid_result.best_index_]))\n",
        "  # means = grid_result.cv_results_['mean_test_score']\n",
        "  # stds = grid_result.cv_results_['std_test_score']\n",
        "  # params = grid_result.cv_results_['params']\n",
        "\n",
        "  #for mean, stdev, param in zip(means, stds, params):\n",
        "  #    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  return grid_result.best_score_,grid_result.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jaD7Y_Ke_1N"
      },
      "outputs": [],
      "source": [
        "def clf_data_svm(df, k_C, k_gamma, df_val = None):\n",
        "\n",
        "  y = df['y'].values\n",
        "  # comenzamos con el modelo de clasificacion\n",
        "  vectorizer = chi2_dim_reduction(df_train, y, n=3) # atentos con el valor de n y el vectorizer\n",
        "\n",
        "  clf = svm.SVC(kernel = 'rbf', C = 2.0**k_C, gamma = 2.0**k_gamma, class_weight='balanced')\n",
        "\n",
        "  # fit sin validacion cruzada\n",
        "\n",
        "  text_features = ColumnTransformer([(\"vectorizer\", vectorizer, 'text_clean')])\n",
        "  model = pipeline.Pipeline([(\"features\", text_features),  (\"classifier\", clf)])\n",
        "\n",
        "  # entrenar y ajustar el clasificador con validacion cruzada\n",
        "\n",
        "  scores = cross_val_score(model, df, y, cv=10, n_jobs = -1)\n",
        "  predicted = cross_val_predict(model, df, y, cv=10, n_jobs=-1)\n",
        "\n",
        "  clf_evaluation(y, predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBuI8kRaCrFW"
      },
      "outputs": [],
      "source": [
        "def plot2_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\t\"\"\"\n",
        "\tThis function prints and plots the confusion matrix.\n",
        "\tNormalization can be applied by setting `normalize=True`.\n",
        "\t\"\"\"\n",
        "\tif normalize:\n",
        "\t\tcm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\t\tprint(\"Normalized confusion matrix\")\n",
        "\telse:\n",
        "\t\tprint('Confusion matrix, without normalization')\n",
        "\n",
        "\tprint(cm)\n",
        "\n",
        "\tplt.imshow(cm, interpolation='nearest', cmap=cmap, aspect='auto')\n",
        "\tplt.title(title)\n",
        "\tplt.colorbar()\n",
        "\ttick_marks = np.arange(len(classes))\n",
        "\tplt.xticks(tick_marks, classes, rotation=45)\n",
        "\tplt.yticks(tick_marks, classes)\n",
        "\n",
        "\tfmt = '.1f' if normalize else 'd'\n",
        "\tthresh = cm.max() / 2.\n",
        "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "\t\tplt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\t\n",
        "\t#plt.tight_layout()\n",
        "\tplt.ylabel('Clase correcta')\n",
        "\tplt.xlabel('Clase predicha')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lZ_1yiFfEQe"
      },
      "outputs": [],
      "source": [
        "def clf_evaluation(y_test, predicted):\n",
        "\n",
        "\t\"\"\"\n",
        "\n",
        "\tFinalmente, vamos a evaluar el desempeño de nuestro clasificador. Calculamos las métricas básicas:\n",
        "\n",
        "\t* Accuracy\n",
        "\t* Precision\n",
        "\t* Recall\n",
        "\t* F1\n",
        "\n",
        "\tAdemás, determinamos nuestra matriz de confusión.\n",
        "\n",
        "\t\"\"\"\n",
        "\tclasses = np.unique(y_test)\n",
        "\n",
        "\t# Accuracy, Precision, Recall\n",
        "\tbalanced_accuracy_score = metrics.balanced_accuracy_score(y_test, predicted)\n",
        "\taccuracy_score = metrics.accuracy_score(y_test, predicted)\n",
        "\tf1_score = metrics.f1_score(y_test, predicted)\n",
        "\tprint(\"Balanced accuracy score:\", balanced_accuracy_score)\n",
        "\tprint(\"Accuracy score:\", accuracy_score)\n",
        "\tprint(\"F1 score:\", accuracy_score)\n",
        "\tprint(\"Details:\")\n",
        "\tprint(metrics.classification_report(y_test, predicted))\n",
        "    \n",
        "\t# Plot confusion matrix\n",
        "\tcfn = metrics.confusion_matrix(y_test, predicted)\n",
        "\tplt.figure()\n",
        "\tplot2_confusion_matrix(cfn, classes=[-1,1], title=('Matriz de Confusión'))\n",
        "\tplt.figure()\n",
        "\tplot2_confusion_matrix(cfn, classes=[-1,1], normalize=True, title=('Matriz de Confusión Normalizada'))\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSwAKj2igmvZ"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07N7dLSWfL5O",
        "outputId": "342012b0-7bf4-47b4-b498-3c76a49f473b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning from data...\n",
            "Iteration #1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n",
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.638075 using {'classifier__C': 0.125, 'classifier__gamma': 0.5, 'classifier__kernel': 'rbf'}\n",
            "Best: 0.638075, 0.028351\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n",
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.777683 using {'classifier__C': 0.3535533905932738, 'classifier__gamma': 1.4142135623730951, 'classifier__kernel': 'rbf'}\n",
            "Best: 0.777683, 0.008998\n",
            "Iteration #2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n",
            "<ipython-input-27-430a185c3cfd>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_features = df_features.append(pd.DataFrame(\n"
          ]
        }
      ],
      "source": [
        "# Empezamos el proceso de clasificación\n",
        "print('Learning from data...')\n",
        "k = 1\n",
        "print('Iteration #'+str(k))\n",
        "prev = 0.0\n",
        "tol = 0.0001\n",
        "search_delta = 0.25\n",
        "best_result, best_params = grid_search_clf(df_test)\n",
        "\n",
        "while (best_result-prev>tol):\n",
        "  prev = best_result\n",
        "  best_result, best_params = grid_search_clf(df, C = np.log2(best_params['classifier__C']), gamma = np.log2(best_params['classifier__gamma']), search_delta = search_delta)\n",
        "  search_delta /= 2\n",
        "  k += 1\n",
        "  print('Iteration #'+str(k))\n",
        "  \n",
        "clf_data_svm(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ndDd6UDb-YXt",
        "Y8Zz6Gc_-cVU",
        "v5dYYWO1-ib7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}